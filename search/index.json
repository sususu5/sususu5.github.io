[{"content":"Adapting HPX\u0026rsquo;s Parallel Algorithms for Usage with Senders and Receivers What is partitioner_with_cleanup in HPX What are Senders and Receivers What is this project about Difficulties and challenges Completed works Future works Acknowledgments This report presents the final outcomes of my Google Summer of Code 2025 project with the Ste||ar Group and serves as complete documentation for anyone interested in the implementation details and results.\nHPX provides a comprehensive implementation of parallel algorithms that serves as both a standards-compliant C++ library and a high-performance computing runtime system. With the upcoming C++26 standard introducing the Senders/Receivers programming model for asynchronous execution, it becomes crucial to ensure HPX\u0026rsquo;s compatibility with it.\nHowever, parallel algorithms based on the partitioner_with_cleanup component currently don\u0026rsquo;t have support for the Senders/Receivers model, this project aims to bridge the gap by adding modification to this component, adjusting these algorithms themselves and providing comprehensive unit tests.\nWhat is partitioner_with_cleanup in HPX The hpx::parallel::util::partitioner_with_cleanup component in HPX library serves as the central execution engine for HPX\u0026rsquo;s parallel algorithms, the functionality of it is mostly the same as hpx::parallel::util::partitioner, but introduces extra cleanup semantic. This critical infrastructure component handles the complex task of dividing work into parallel chunks, managing their execution across different policies, and ensuring robust error handling and resource cleanup.\nTo be specific, the running process of partitioner_with_cleanup can be divided into the following steps:\nBind policy/executor parameters. Build partition shape and bulk-launch chunk work (f1) on executor. Reduce results (f2) while applying error handling. On any failure, run cleanup on successful chunks to roll back. Return result type dictated by policy Within the partitioner_with_cleanup component, HPX provides two distinct implementations to handle different execution models. The static_partitioner_with_cleanup is used for non-task execution policies, including sequenced/parallel policies and Sender/Receiver execution. In contrast, the task_static_partitioner_with_cleanup is used for task-based execution policies such as sequenced_task and parallel_task.\nIn the call chain, there are two main functions, which are the call function and reduce function, they are responsible for the partitioning and reducing process respectively.\nThe call function takes the following parameters: an execution policy policy, a range (a forward iterator first and a std::size_t value count), a per-chunk worker f1, a final reducer f2, and a cleanup function cleanup. This function firstly partitions the range into chunks according to the execution policy, calling f1 on all chunks, then call the reduce function to merge the algorithm results.\n1 2 3 4 template \u0026lt;typename ExPolicy_, typename FwdIter, typename F1, typename F2, typename Cleanup\u0026gt; static decltype(auto) call(ExPolicy_\u0026amp;\u0026amp; policy, FwdIter first, std::size_t count, F1\u0026amp;\u0026amp; f1, F2\u0026amp;\u0026amp; f2, Cleanup\u0026amp;\u0026amp; cleanup) The reduce function takes three parameters: a list of chunk results workitems, a reducing function f, and a cleanup function cleanup, This function firstly waits for all workitems to complete. If there is any exception returned by workitems, the cleanup function will be called on all successful results and the captured exceptions are thrown again. If all the workitems are successful, a result of calling the reduce function will be returned.\n1 2 template \u0026lt;typename Items, typename F, typename Cleanup\u0026gt; static decltype(auto) reduce(Items\u0026amp;\u0026amp; workitems, F\u0026amp;\u0026amp; f, Cleanup\u0026amp;\u0026amp; cleanup) If you are interested in more details about HPX\u0026rsquo;s parallel algorithms, a good reference is Tobias Wukovitsch\u0026rsquo;s project of Google Summer of Code 2024, which includes a clear and detailed introduction.\nWhat are Senders and Receivers Since this project is about Senders/Receivers, it\u0026rsquo;s important to provide a brief introduction to it.\nModern C++ has long lacked a standard way to express asynchronous operations. Existing approaches such as futures, callbacks, or third-party libraries often suffer from inconsistent semantics or limited support for cancellation and error handling, which may cause potential issues.\nHence, the C++ standards committee has been working on a proposal known as P2300: std::execution, which introduces a new abstraction for asynchronous programming called the Senders/Receivers model. This proposal is expected to form the foundation of future concurrency support in the C++ standard.\nAt its core, the Senders/Receivers model separates two roles: the Sender, which produces an asynchronous result, and the Receiver, which consumes it. A sender does not run work immediately but describes what will be done once execution starts.\nTo execute a sender, it should be firstly connected to a receiver through connect(), which forms an operation state, this object knows what to do and where the result should be given to. Then, start() must be called on it to trigger the actual asynchronous execution.\nWhen the work completes, the sender signals the receiver in one of three ways:\nValue → the operation completed successfully and produced a result Error → the operation failed and communicates the failure downstream Stopped → the operation was cancelled before producing a result This tri-state completion model makes error handling and cancellation first-class citizens rather than afterthoughts. It also allows asynchronous operations to be composed in a consistent and declarative style.\nIn the exact codebase, developers often uses functions like sync_wait(), when_all(), and let_value() to implicitly complete the connect() and start() operations. Here is a simple example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #include \u0026lt;stdexec/execution.hpp\u0026gt; #include \u0026lt;iostream\u0026gt; int main() { // Create a thread pool and get its scheduler stdexec::static_thread_pool pool{2}; auto scheduler = pool.get_scheduler(); auto s = stdexec::just(42) // just() describes \u0026#34;generate 42\u0026#34; | stdexec::continue_on(scheduler) // continue_on() switches execution to the pool\u0026#39;s scheduler | stdexec::then([](int x){ return x * 2; }); // then() describes \u0026#34;times x by 2\u0026#34; // sync_wait() explicitly connects a receiver and starts it auto [result] = stdexec::sync_wait(s).value(); std::cout \u0026lt;\u0026lt; \u0026#34;Result = \u0026#34; \u0026lt;\u0026lt; result \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; // prints: 84 } What is this project about The primary goal of this project is to extend HPX’s parallel algorithms so that they fully support the Senders/Receivers (S/R) model when built on top of the partitioner_with_cleanup component. The work is organized into three parts:\nExtending partitioner_with_cleanup\nIntroduce logic branches to handle calls originating from S/R-based execution, covering both launch policies and execution policies. These adaptations ensure clean integration with the S/R model while preserving existing semantics.\nAdapting algorithm implementations\nModify affected algorithms to align with S/R usage. To be specific, changing the return type from algorithm_result to decltype(auto) so that senders are correctly deduced as return values. Additionally, disable early exits in the S/R branch to keep sender chains intact and faithfully represent the asynchronous workflow.\nDeveloping unit tests\nProvide a comprehensive suite of tests for the affected algorithms. Each test evaluates four combinations of launch and execution policies:\nhpx::launch::sync with seq(task) hpx::launch::sync with unseq(task) hpx::launch::async with seq(task) hpx::launch::async with unseq(task) These combinations cover major usage scenarios and mirror test patterns used elsewhere in HPX, ensuring consistency and reliability.\nDifficulties and challenges The key challenge of this project is how to modify the partitioner_with_cleanup component so that it can handle failures gracefully. While the regular partitioner component can assume everything will work perfectly and simply return the reduced results, partitioner_with_cleanup must be more cautious. It needs to track which subtasks succeed or fail, and more importantly, it must call cleanup functions for successful work when other parts of the operation fail.\nAs mentioned above, an asynchronous operation executed in the Senders/Receivers model can finish in three distinct ways, which are Value, Error, and Stopped. Therefore, if any chunk fails, the partitioning function call completes on the error channel. Consequently, the reduce function can only observe an error signal rather than all the chunk results from the algorithm, making it impossible to identify which chunks completed successfully and preventing proper cleanup function from being called.\nTherefore, we need to distinguish between normal results and errors during execution. To achieve this, the call function wraps the original algorithm parameter f1 in a lambda expression, ensuring that every return value is stored in a variant type. Each partition task therefore produces either a normal Result or an exception_ptr.\nNext, the reduce function linearly traverses all partition results. If it detects an error, it first releases the resources held by already successful subtasks, and then rethrows the exception.\nThis pseudocode illustrates the principle, actual implementation differs in template details.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // Here is a simplified version of the above idea // 1. Wrap f1 so each chunk returns either Result or exception_ptr auto wrapped_f1 = [\u0026amp;](auto\u0026amp;\u0026amp;... args) -\u0026gt; std::variant\u0026lt;Result, std::exception_ptr\u0026gt; { try { return f1(std::forward\u0026lt;decltype(args)\u0026gt;(args)...); } catch (...) { return std::current_exception(); } }; // 2. Partition returns a collection of variant results auto items = partition\u0026lt;std::variant\u0026lt;Result, std::exception_ptr\u0026gt;\u0026gt;( policy, first, count, wrapped_f1); // 3. Reduce traverses all items, cleans up successes, rethrows on error for (auto\u0026amp; item : items) { if (std::holds_alternative\u0026lt;std::exception_ptr\u0026gt;(item)) { cleanup_successful(items); std::rethrow_exception(std::get\u0026lt;std::exception_ptr\u0026gt;(item)); } } In this way, although each subtask passes its result by value, the final Sender always completes in a successful state by returning a container of variants. The caller can then inspect this container to uniformly detect errors, perform cleanup, and rethrow exceptions if necessary.\nCompleted works This project successfully integrated Senders/Receivers model support into all parallel algorithms that utilize the partitioner_with_cleanup component, accompanied by comprehensive unit test coverage.\nMemory Construction Algorithms:\nuninitialized_copy, uninitialized_copy_n uninitialized_default_construct, uninitialized_default_construct_n uninitialized_fill, uninitialized_fill_n uninitialized_value_construct, uninitialized_value_construct_n Memory Movement Algorithms:\nuninitialized_move, uninitialized_move_n uninitialized_relocate, uninitialized_relocate_backward, uninitialized_relocate_n All implementation details, code modifications, and test cases can be reviewed in PR #6741, which contains the complete set of changes of project.\nFuture Work While this project has successfully extended Senders/Receivers model support to more of HPX\u0026rsquo;s parallel algorithms, several algorithms remain unadapted. Remaining work items are tracked in this table.\nIn the future, development priority should include adapting all parallel algorithms to the Senders/Receivers model, optimizing the performance of these algorithms by fine-tuning the implementation and enhancing test cases.\nAcknowledgments I would like to express my sincere gratitude to Hartmut Kaiser, Isidoros Tsaousis-Seiras and Panos Syskakis for their invaluable guidance and continuous support throughout this project. Their expertise was instrumental in helping me navigate complex technical challenges.\nI\u0026rsquo;m also grateful to Tobias Wukovitsch for his excellent work and report in Google Summer of Code 2024, which provided essential background for this work.\nFinally, I thank Google and the Google Summer of Code program for providing this incredible opportunity to contribute to open-source software and grow as a developer.\n","date":"2025-08-29T18:23:46+10:00","permalink":"https://sususu5.github.io/p/google-summer-of-code-final-report/","title":"Google Summer of Code Final Report"},{"content":"How to Implement a C++ Vector From Scratch Introduction std::vector is a sequence container in C++ that represents a dynamic array. It is part of the C++ Standard Template Library (STL) and provides the ability to store and manipulate a collection of elements with automatic memory management and flexible resizing.\nstd::vector is the most commonly used container in C++ development, whether you are passionate about competitive coding or interested in C++ programming, learning the low-level implementation of it helps you gain a more thorough understanding of C++ language and prepares you for better using it in the future.\nPrerequisite Knowledge Rule of Five The Rule of Five in C++ is a guideline helping developers manage resource ownership in user-defined types.\nTo be specific, if your class manages a resource, and you define any one of the following five special member functions, you should probably define all five to ensure correct behavior:\nDestructor Copy Constructor Copy Assignment Operator Move Constructor Move Assignment Operator reinterpret_cast\u0026lt;T\u0026gt;(expression) reinterpret_cast is a low-level type conversion operator in C++ used to reinterpret the binary representation of a value as a different type.\nTwo important thing about it is that it does not perform any type safety checks and it simply treats the bits of one type as if they were another.\n::operator new and ::operator delete ::operator new is a built-in global function in C++ that allocates raw, uninitialized memory on the heap.\nIt is not the same as the new expression, though the new expression uses it internally. The :: scope resolution prefix ensures you’re calling the global version of operator new, avoiding any class-specific overloads.\n::operator delete is the global deallocation function to free raw memory that was previously allocated using ::operator new.\nStep by Step Implementation Private Fields To implement a std::vector, firstly we need to define the private fields of this class.\n1 2 3 T* data_ = nullptr; // data_ is a pointer pointing to a generic type T size_t capacity_ = 0; size_t size_ = 0; Additionally, we need to define the initial size of a vector and the growth factor. The value of a growth factor is the growing rate of a vector once the capacity is full.\n1 2 static constexpr size_t startSize_ = 10; static constexpr double growthFactor_ = 1.6; // if growthFactor = 2, the vector will be doubled when it\u0026#39;s full Constructors and Destructor According to Rule of Five, we need to implement three types of constructors for out vector class, which are Default Constructor, Copy Constructor, and Move Constructor.\nIn Default Constructor, we need to allocate initial capacity memory for the underlying data and initialzie the member variables.\n1 2 3 4 Vector() { data_ = reinterpret_cast\u0026lt;T*\u0026gt;(::operator new(startSize_ * sizeof(T))); capacity_ = startSize_; } In Copy Constructor, we need to allocate capacity for a new vector and keep the input object unchanged.\n1 2 3 4 5 6 Vector(const Vector\u0026amp; other) : size_(other.size_), capacity_(other.capacity_) { data_ = reinterpret_cast\u0026lt;T*\u0026gt;(::operator new(capacity_ * sizeof(T))); for (size_t i = 0; i \u0026lt; size_; ++i) { new (data_ + i) T(other.data_[i]); } } In Move Constructor, we need to transfer the ownership of resources from one object to another instead of copying, which means the input object of this constructor should be set to void.\n1 2 3 4 5 6 Vector(Vector\u0026amp;\u0026amp; other) noexcept : data_(other.data_), capacity_(other.capacity_), size_(other.size_) { other.data_ = nullptr; other.capacity_ = 0; other.size_ = 0; } In the Destructor\n1 2 3 4 5 6 ~Vector() { for (size_t i = 0; i \u0026lt; size_; ++i) { data_[i].~T(); } ::operator delete(data_); } Assignments According to the Rule of Five, both the copy assignemnt and the move assignemnt must be implemented for this class.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // Copy assignment Vector\u0026amp; operator=(const Vector\u0026amp; other) { if (this != \u0026amp;other) { Vector temp(other); swap(temp); } return *this; } // Move assignment Vector\u0026amp; operator=(Vector\u0026amp;\u0026amp; other) noexcept { if (this != \u0026amp;other) { Vector temp(std::move(other)); swap(temp); } return *this; } Out of convenience, we can also define a specific swap function, which swaps another object with this.\n1 2 3 4 5 void swap(Vector\u0026amp; other) noexcept { std::swap(data_, other.data_); std::swap(capacity_, other.capacity_); std::swap(size_, other.size_); } Element Insertion In std::vector, the push_back method appends a new element to the end of the container and there are two versions should be implemented, which are the one handling the left value\n1 2 3 4 5 6 7 8 9 10 void push_back(const T\u0026amp; elem) { if (size_ == capacity_) { reserve(capacity_ ? static_cast\u0026lt;size_t\u0026gt;(capacity_ * growthFactor_) : startSize_); assert(size_ \u0026lt; capacity_); } assert(data_); new (data_ + size_) T(elem); ++size_; } and the one handling the right value.\n1 2 3 4 5 6 7 8 9 10 void push_back(T\u0026amp;\u0026amp; elem) { if (size_ == capacity_) { reserve(capacity_ ? static_cast\u0026lt;size_t\u0026gt;(capacity_ * growthFactor_) : startSize_); assert(size_ \u0026lt; capacity_); } assert(data_); new (data_ + size_) T(std::move(elem)); ++size_; } Element Access To allow users to access elements by index, we need to overload two versions of operator[], which handle both const and non-const objects.\n1 2 3 4 5 6 7 8 9 const T\u0026amp; operator[](size_t idx) const { assert(idx \u0026lt; size_); return data_[idx]; } T\u0026amp; operator[](size_t idx) { assert(idx \u0026lt; size_); return data_[idx]; } In addition, common operations like front, back, and size should also be added, which are\n1 2 3 4 5 6 7 8 9 10 11 12 13 const T\u0026amp; front() const { assert(size_ \u0026gt; 0); return operator[](0); } const T\u0026amp; back() const { assert(size_ \u0026gt; 0); return operator[](size_ - 1); } size_t size() const { return size_; } size_t capacity() const { return capacity_; } bool empty() const { return size_ == 0; } Element Removal \u0026amp; Container Modification There are three operations to remove elemnt from a vector, which are pop_back, erase, and clear.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 // Remove element at specific index void erase(size_t idx) { if (idx \u0026gt;= size_) return; // Destroy the element at idx data_[idx].~T(); // Move elements to fill the gap for (size_t j = idx; j \u0026lt; size_ - 1; ++j) { new (data_ + j) T(std::move(data_[j + 1])); data_[j + 1].~T(); } --size_; } // Remove all elements in the vector void clear() { for (size_t i = 0; i \u0026lt; size_; ++i) { data_[i].~T(); } size_ = 0; } // Remove an element at the back of this vector void pop_back() { if (size_ \u0026gt; 0) { --size_; data_[size_].~T(); } } Memory Management The reserve operation is used to pre-allocate memory for a container, which is useful when you know how many elements will be added to a vector.\n1 2 3 4 5 6 7 8 9 10 11 12 void reserve(size_t newCapacity) { if (capacity_ \u0026gt;= newCapacity) return; auto* newData = reinterpret_cast\u0026lt;T*\u0026gt;(::operator new(newCapacity * sizeof(T))); for (size_t i = 0; i \u0026lt; size_; ++i) { new (newData + i) T(std::move(data_[i])); data_[i].~T(); } ::operator delete(data_); data_ = newData; capacity_ = newCapacity; } Complete Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 #include \u0026lt;cstddef\u0026gt; #include \u0026lt;cassert\u0026gt; #include \u0026lt;cstdlib\u0026gt; #include \u0026lt;utility\u0026gt; #include \u0026lt;new\u0026gt; #include \u0026lt;vector\u0026gt; template \u0026lt;typename T\u0026gt; class Vector { public: // Default constructor Vector() { data_ = reinterpret_cast\u0026lt;T*\u0026gt;(::operator new(startSize_ * sizeof(T))); capacity_ = startSize_; } // Copy constructor Vector(const Vector\u0026amp; other) : size_(other.size_), capacity_(other.capacity_) { data_ = reinterpret_cast\u0026lt;T*\u0026gt;(::operator new(capacity_ * sizeof(T))); for (size_t i = 0; i \u0026lt; size_; ++i) { new (data_ + i) T(other.data_[i]); } } // Move constructor Vector(Vector\u0026amp;\u0026amp; other) noexcept : data_(other.data_), capacity_(other.capacity_), size_(other.size_) { other.data_ = nullptr; other.capacity_ = 0; other.size_ = 0; } // Copy assignment Vector\u0026amp; operator=(const Vector\u0026amp; other) { if (this != \u0026amp;other) { Vector temp(other); swap(temp); } return *this; } // Move assignment Vector\u0026amp; operator=(Vector\u0026amp;\u0026amp; other) noexcept { if (this != \u0026amp;other) { Vector temp(std::move(other)); swap(temp); } return *this; } // Destructor ~Vector() { for (size_t i = 0; i \u0026lt; size_; ++i) { data_[i].~T(); } ::operator delete(data_); } // Left value reference void push_back(const T\u0026amp; elem) { if (size_ == capacity_) { reserve(capacity_ ? static_cast\u0026lt;size_t\u0026gt;(capacity_ * growthFactor_) : startSize_); assert(size_ \u0026lt; capacity_); } assert(data_); new (data_ + size_) T(elem); // Fixed: construct at correct position with elem ++size_; } // Right value reference void push_back(T\u0026amp;\u0026amp; elem) { if (size_ == capacity_) { reserve(capacity_ ? static_cast\u0026lt;size_t\u0026gt;(capacity_ * growthFactor_) : startSize_); assert(size_ \u0026lt; capacity_); } assert(data_); new (data_ + size_) T(std::move(elem)); ++size_; } // Const value overload const T\u0026amp; operator[](size_t idx) const { assert(idx \u0026lt; size_); return data_[idx]; } // Non-const value overload T\u0026amp; operator[](size_t idx) { assert(idx \u0026lt; size_); return data_[idx]; } const T\u0026amp; front() const { assert(size_ \u0026gt; 0); return operator[](0); } const T\u0026amp; back() const { assert(size_ \u0026gt; 0); return operator[](size_ - 1); } size_t size() const { return size_; } size_t capacity() const { return capacity_; } bool empty() const { return size_ == 0; } void reserve(size_t newCapacity) { if (capacity_ \u0026gt;= newCapacity) return; auto* newData = reinterpret_cast\u0026lt;T*\u0026gt;(::operator new(newCapacity * sizeof(T))); for (size_t i = 0; i \u0026lt; size_; ++i) { new (newData + i) T(std::move(data_[i])); data_[i].~T(); } ::operator delete(data_); data_ = newData; capacity_ = newCapacity; } void erase(size_t idx) { if (idx \u0026gt;= size_) return; // Destroy the element at idx data_[idx].~T(); // Move elements to fill the gap for (size_t j = idx; j \u0026lt; size_ - 1; ++j) { new (data_ + j) T(std::move(data_[j + 1])); data_[j + 1].~T(); } --size_; } void clear() { for (size_t i = 0; i \u0026lt; size_; ++i) { data_[i].~T(); } size_ = 0; } void pop_back() { if (size_ \u0026gt; 0) { --size_; data_[size_].~T(); } } private: void swap(Vector\u0026amp; other) noexcept { std::swap(data_, other.data_); std::swap(capacity_, other.capacity_); std::swap(size_, other.size_); } T* data_ = nullptr; size_t capacity_ = 0; size_t size_ = 0; static constexpr size_t startSize_ = 10; static constexpr double growthFactor_ = 1.6; }; int main() { Vector\u0026lt;int\u0026gt; v; assert(v.size() == 0); for (int i = 0; i \u0026lt; 100; ++i) v.push_back(i); for (int i = 0; i \u0026lt; 100; ++i) assert(v[i] == i); assert(v.size() == 100); for (int i = 0; i \u0026lt; 50; ++i) { assert(v[0] == i); v.erase(0); } assert(v.size() == 50); for (int i = 50; i \u0026lt; 100; ++i) { assert(v[0] == i); v.erase(0); } assert(v.size() == 0); Vector\u0026lt;int\u0026gt; v2; v2.push_back(42); assert(v2.front() == 42); assert(v2.back() == 42); Vector\u0026lt;int\u0026gt; v3 = v2; assert(v3.size() == 1); assert(v3[0] == 42); return 0; } ","date":"2025-06-11T13:51:00+10:00","permalink":"https://sususu5.github.io/p/how-to-implement-a-c-vector-from-scratch/","title":"How to Implement a C++ Vector From Scratch"},{"content":"How to design a POI (Point of Interest) Service Functional Requirements Searching nearby locations Viewing details Owners can edit / delete Non-functional Requirements Latency: 1s Freshness: 1 day -\u0026gt; 1 hour Scalability Fault Tolerance Assumptions 1B users -\u0026gt; 50% -\u0026gt; 500M DAU 200M businesses Storage: NoSQL, SQL, in memory Database Selection In-memory: using multiple machines to shard the data, and the latency requirement is 1s, it\u0026rsquo;s expensive to use this SQL: more expensive NoSQL: cheaper, fast querying, self-sharding and optimization, suitable How to scale searching Master-Slave Model, separate reading and writing This system is suitable for using this model because the need for reading is much more than writing Not using whole table scaning here, given longtitude and latitude Composite Index can not speed up the searching here because it follows the Leftmost Prefix Rule, we can only search the range of the latitude once the longitude is matched, so it equals whole table scaning Two other effective searching ways will be introduced in the following content GeoHash GeoHash is a spatial hashing algorithm that continuously divides latitude and longitude ranges in half (using a Z-order curve), converting a (latitude, longitude) pair into a Base32 string that represents the geographic grid cell in which the location falls It is a value stored in the database The longer the string, the higher the precision of the geographical location Quad Tree A Quad Tree is a tree data structure that recursively partitions a two-dimensional space into four quadrants (subregions). Each node in the tree has up to four children representing the top-left, top-right, bottom-left, and bottom-right regions It is a memory structure Quad Tree can be used as indicies to speed up the searching (k values), assume each node contains about 100 businesses Can it fits into memory? (business_id, latitude, longitute, short_brief) \u0026lt; 50B (200M / 100) * (100 * 50B) = 10GB, which can be stored in memory, meets the latency requirement in 1 second\nHow to build this tree? Need a starting point, stored on disk as a seed value\nHow to update this tree? Live update / eventually consistecy\nAdding a cache to store the latitude, longitude, and their corresponding business list, extra cost.\nConstruct the Quad Tree according to the deployment (west, middle, east USA), extra cost, availability\n","date":"2025-06-07T11:43:38+10:00","permalink":"https://sususu5.github.io/p/how-to-design-a-poi-point-of-interest-service/","title":"How to Design a POI (Point of Interest) Service"},{"content":"How to design a TikTok Functional Requirements Video Upload: Allow users to upload videos to the platform. Video Playback: Enable users to watch videos. By developing these, we can construct a minimum viable product.\nNon-functional Requirements Scalability: Support a massive user base with high concurrency. Availability: Ensure the system remains operational even during partial failures. Low Latency: Minimize delays in video loading and playback. Fault Tolerance: Handle hardware/network failures gracefully without data loss. Assumptions User Base: 1 billion daily active users (DAU). Usage Patterns: Each user watches 100 videos per day. Each user uploads 1 video per day. Video Size: Average video size is 10MB. Database Selection: SQL vs NoSQL SQL Databases: Pros: Strong consistency, relational data support. Cons: Challenges with sharding and hotspot management. NoSQL Databases: Pros: Cost-effective, horizontally scalable. Cons: Limited transactional support. Video Storage Strategy Blob Storage (Binary Large Object):\nOptimized for unstructured data (videos, images, audio). Ideal for storing and retrieving large volumes of small files efficiently. How to upload a video Since we don\u0026rsquo;t know what users are uploading, exposing the storage to the interface directly is unsafe. A better option is allocating a temporary space to store the original videos uploaded by users. When uploading, a video can be cut into small pieces to support breakpoint resume upload when a break happens, and also parallel uploading, which means multiple segments can be uploaded simultaneously. (for a mobile app, the network environment is not stable) Once all segments are uploaded, we can use a message queue and a worker pool to merge all the segments and do a file integrity verification. After that, this video should be encoded into different formats because videos of different qualities should be played according to devices and network. How to watch a video To avoid hotspots caused by frequent access to popular videos, we can deploy a CDN near user locations to offload traffic from the blob storage. Although storing videos in a CDN speeds up delivery and reduces latency, it also comes with high costs. So, we should make sure only the most popular videos are cached there. By introducing an extractor service, it can regularly find popular videos from the blob storage and send them to the CDN, those outdated videos are replaced. We can also introduce a streaming protocol like the HTTP Live Streaming from Apple to realize \u0026ldquo;stream-as-you-go\u0026rdquo; manner, which improves user experience. Show off We can introduce a recommendation system to recommend videos to users rather than pushing original feed accoring to the time. We can introduce a Two-Tower Model to embed user and video features into separate vectors. When a client requests videos, the system can recommend those that match their interests based on vector similarity. The pro is the vieo watching time of clients can be extended, the con is the extra cost of hiring a team to construct and deploy the model. ","date":"2025-06-03T16:18:56+10:00","permalink":"https://sususu5.github.io/p/how-to-design-a-tiktok/","title":"How to Design a TikTok"}]