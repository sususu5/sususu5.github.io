[{"content":"How to Design a Chat System at Scale Requirements Functional Requirements One-to-One Chat: Real-time online message delivery. Reliable offline message storage and retrieval. Group Chat: Support for small to medium groups (\u0026lt; 200 members). Presence Status: Real-time indicators for Online/Offline/Busy states. Non-functional Requirements Scalability: Support 1 Billion registered users. Low Latency: End-to-end latency \u0026lt; 500ms (aiming for near real-time). High Consistency: Messages must be delivered in order (causal consistency) and exactly once. Fault Tolerance: The system must be resilient to server failures without data loss. Communication Protocols (Message Model) To achieve low latency and bi-directional communication, we compare the following mechanisms:\nPolling: Client periodically requests updates. Inefficient due to high overhead and latency. Long Polling: Client holds connection until data is available. Better than polling, but still resource-heavy on server headers/handshakes. WebSocket (Recommended): Persistent, full-duplex TCP connection. Ideal for real-time messaging. QUIC / HTTP/3: UDP-based transport with congestion control. Excellent for unstable networks (mobile clients) and reducing head-of-line blocking. Decision: WebSocket is the industry standard for the connection layer due to its widespread support and efficiency for stateful connections.\nArchitecture \u0026amp; Scalability Connection Management (Session Affinity) To ensure low latency, we need a stateful connection layer (Gateway/Chat Server). A user\u0026rsquo;s WebSocket connection must persist on a specific server.\nService Discovery \u0026amp; Session Storage:\nWe use a centralized Key-Value store (e.g., Redis) to map users to their physical servers.\nData Structure: \u0026lt;user_id, {last_heartbeat: timestamp, chat_server_id, ...}\u0026gt; Workflow: When User A wants to send a message to User B, the router looks up User B\u0026rsquo;s chat_server_id in Redis to forward the payload. Capacity Planning (Memory for Sessions):\nTotal Users: 1B Daily Active Users (DAU): 1B * 20% = 200M Peak Concurrent Users: 200M * 25% = 50M Memory Usage per User Session: approx 100 bytes (fd, ip_addr, port, state, user_id\u0026hellip;) Total Memory Required: 50M * 100 bytes = 5GB Note: 5GB fits easily into the RAM of a modern Redis cluster.\nMessage Synchronization \u0026amp; Throughput To decouple message ingestion from delivery and storage, we introduce a Message Queue (e.g., Kafka) between the connection layer and the backend logic.\nThroughput Calculation (Peak Time):\nAverage message rate: 0.2 msg/sec/user Total Throughput: 50M users * 0.2 = 10M msg/s Message Size: Header (IDs): 8 * 3 = 24 bytes Content: 600-800 bytes Total (Compressed): approx 0.5KB Bandwidth: 10M msg/s * 0.5KB = 5GB/s Scaling Strategy:\nSharding: Since 5GB/s is high for a single cluster, we must shard the Message Queue by user_id (or group_id for groups) to ensure ordering while distributing load. Server Count: Benchmark: WhatsApp (Erlang-based) achieved 2M concurrent connections per server. Required Chat Servers: 50M / 2M = 25 Servers. Result: This is highly manageable with modern C++ asynchronous I/O (epoll/io_uring). Global Distribution (Edge Aggregation) To handle the 5GB/s global traffic and reduce latency for cross-region users, we use Edge Aggregation.\nMechanism: Users connect to the nearest Edge Node. Edge Nodes aggregate messages and send batch requests to the Central Data Center (or cross-region sync). Edge Capacity: Assume one region handles 2M users. Traffic: 2M * 0.2 * 0.5KB = 200MB/s. Feasibility: A single modern server with 10Gbps NIC can handle this comfortably. Group Chat Design Group chats introduce a \u0026ldquo;Fan-out\u0026rdquo; problem (1 sender, N receivers).\nData Model:\n1 2 3 4 5 6 7 { \u0026#34;group_id\u0026#34;: 1001, \u0026#34;message_id\u0026#34;: 987654321, \u0026#34;sender_id\u0026#34;: 123, \u0026#34;content\u0026#34;: \u0026#34;Hello World\u0026#34;, \u0026#34;timestamp\u0026#34;: 1700000000 } Routing Strategy:\nInstead of broadcasting to all users immediately, we use a Group Register/Service.\nGroup Registration: Records which Chat Servers host online members of a specific group. Delivery: The message is routed only to those specific Chat Servers. Optimization: For large groups, limit the fan-out by only pushing notifications to active members, while others pull on demand. Architectural Patterns:\nPush Model / Write Fanout Suitable for small groups and real-time communication. Mechanism: When a user sends a message to a group, the message is pushed to all online members of the group. Pros: Fast read operations, users only need to read from their inbox. Cons: High write overhead, the more members in a group, the more messages need to be pushed. Pull Model / Read Fanout Suitable for large groups and offline messages. Mechanism: When a user sends a message to a group, the message is pushed to the group register/service, and the members will pull the messages from the group register/service on demand. Pros: Fast write operations, users only need to write to the group register/service. Cons: High read overhead, the more members in a group, the more messages need to be pulled. Storage \u0026amp; Offline Buffering We need a tiered storage strategy: fast write for recent messages and cost-effective storage for history.\nCapacity Planning (Storage):\nTotal Messages/Day: 200M DAU * 40 msg/user = 8B messages Offline Message Ratio: 15% Daily Storage: 8B * 15% * 0.5KB = 0.6TB / Day Monthly Storage: 0.6TB * 30 = 18TB Technology Choice:\nHot Data (Recent): NoSQL (Cassandra / DynamoDB / HBase). Optimized for write-heavy workloads and time-series data. Cold Data (History): Object Storage (S3) or Compressed Archives. Offline Buffer: Redis Streams or temporary KV storage to hold messages until the user comes online. ","date":"2025-12-24T16:38:41+08:00","permalink":"https://sususu5.github.io/p/how-to-design-a-chat-system/","title":"How to Design a Chat System"},{"content":"This is my personal notes of the talk \u0026ldquo;When Nanoseconds Matter: Ultrafast Trading Systems in C++\u0026rdquo; from Cppcon.\nOrder Book Properties Two ordered sequences Bids: from highest to lowest price Asks: from lowest to highest price Price Level Price Size: sum of all orders\u0026rsquo; size at this price level Each order has an ID(uint64_t) which is unqiue throughout the trading session A typical tock order book has ~1000 price level \u0026ldquo;per side\u0026rdquo; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 std::map\u0026lt;Price, Volume, std::greater\u0026lt;Price\u0026gt;\u0026gt; mBidLevels; std::map\u0026lt;Price, Volume, std::less\u0026lt;Price\u0026gt;\u0026gt; mAskLevels; template \u0026lt;class T\u0026gt; typename T::iterator AddOrder(T\u0026amp; levels, Price price, Volume volume) { auto [it, inserted] = levels.try_emplace(price, volume); if (inserted == false) it-\u0026gt;second += volume; return it; } template \u0026lt;class T\u0026gt; void DeleteOrder(typename T::iterator it, T\u0026amp; levels, Price price, Volume volume) { it-\u0026gt;second -= volume; if (it-\u0026gt;second \u0026lt;= 0) levels.erase(it); } Principle #1: \u0026ldquo;Most of the time, you don\u0026rsquo;t want node containers\u0026rdquo;\nUsing std::vector Using two vectors (bids and asks) and use std::lower_bound (binary search).\n1 2 std::vector\u0026lt;std::pair\u0026lt;Price, Volume\u0026gt;\u0026gt; mBidLevels; std::vector\u0026lt;std::pair\u0026lt;Price, Volume\u0026gt;\u0026gt; mAskLevels; AddOrder log(N) if the price level exists log(N) + N if a new price level is inserted ModifyOrder: log(N) We can\u0026rsquo;t store iterators/pointers as they are being invalidated on a call to std::vector::insert DeleteOrder log(N) if the price level exists log(N) + N if the price level does not exist 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 void AddOrder(Side side, Price price, Volume volume) { if (side == Side::Bid) return AddOrder(bidLevels, price, volume, std::greater\u0026lt;Price\u0026gt;()); else return AddOrder(askLevels, price, volume, std::less\u0026lt;Price\u0026gt;()); } template \u0026lt;class T, class Compare\u0026gt; void AddOrder(T\u0026amp; levels, Price price, Volume volume, Compare comp) { auto it = std::lower_bound(levels.begin(), levels.end(), price, [comp](const auto\u0026amp; p, Price price) { return comp(p.first, price); }); if (it != levels.end() \u0026amp;\u0026amp; it-\u0026gt;first == price) it-\u0026gt;second += volume; else levels.insert(it, {price, volume}); } \u0026ldquo;reverse\u0026rdquo; vector 1 2 3 4 5 // Reversing the vector to minimize the number of elements being moved auto GetBestPrices() const { return {mBidLevels.rbegin().first, mAskLevels.rbegin().first}; } Principle #2: \u0026ldquo;Understanding your problem (by looking at data!)\u0026rdquo;\nPrinciple #3: \u0026ldquo;Hand tailored (specialized) algorithms are key to achieve performance\u0026rdquo;\nRunning perf on a benchmark Start perf once the initialization is completed so that the exact benchmark is assessed without the cost of initialization.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 void RunPerf() { pid_t pid = fork(); if (pid == 0) { const auto parentPid = std::to_string(getppid()); std::cout \u0026lt;\u0026lt; \u0026#34;Running perf on parent process \u0026#34; \u0026lt;\u0026lt; parentPid \u0026lt;\u0026lt; std::endl; // Changing the current process into perf execlp(\u0026#34;perf\u0026#34;, \u0026#34;perf\u0026#34;, ..., parentPid.c_str(), (char*)nullptr); throw std::runtime_error(\u0026#34;execlp failed\u0026#34;); } } void InitAndRunBenchmark() { InitBenchmark(); // might take a long time! RunPerf(); RunBenchmark(); } First perf measurements should never be too specific.\nBranchless binary search Because of branch prediction, if a misprediction happens, the pipeline will be flushed, and the CPU needs to read commands from the correct address again, which is a waste of 10-20 clock cycles.\nBy using branchless logic, the waste of clock cycles is avoided.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 template \u0026lt;class ForwardIt, class T, class Compare\u0026gt; ForwardIt branchless_lower_bound(ForwardIt first, ForwardIt last, const T\u0026amp; value, Compare comp) { auto length = last - first; while (length \u0026gt; 0) { auto half = length / 2; // multiplication (by 1) is needed for GCC to generate CMOV first += comp(first[half], value) * (length - half); length = half; } return first; } Linear search is fast because of better cache locality.\nPrinciple #4: \u0026ldquo;Simplicity is the ultimate sopistication\u0026rdquo;\nPrinciple #5: \u0026ldquo;Mechanical sympathy\u0026rdquo;\nI-Cache misses - likely/unlikely attributes I-Cache misses - Immediately Invoked Function Expressions(IIFE) Lambda, Functor vs std::function Transport: networking \u0026amp; concurrency Userspace Networking Solarflare OpenOnload TCPDirect EF_VI Principle #6: \u0026ldquo;True efficiency is found not in the layers of complexity we add, but in the unnecessary layers we remove\u0026rdquo;\nShared Memory Concurrent Queues Principle #7: \u0026ldquo;Choose the right tool for the right task\u0026rdquo;\nFastQueue - Design 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 struct FastQueue { // Both counters are written by the producer, read by the consumer(s) // Before/after a Write operation, both counters contain the same value // [WriteCounter, ReadCounter] defines the area of reading alignas(CACHE_LINE_SIZE) std::atomic\u0026lt;uint64_t\u0026gt; mReadCounter{0}; // [ReadCounter, WriteCounter] defines the area of writing alignas(CACHE_LINE_SIZE) std::atomic\u0026lt;uint64_t\u0026gt; mWriteCounter{0}; alignas(CACHE_LINE_SIZE) uint8_t mBuffer[0]; }; // simplified code! void QProducer::Write(std::span\u0026lt;std::byte\u0026gt; buffer) { const int32_t payloadSize = sizeof(int32_t) + buffer.size(); mLocalCounter += payloadSize; mQ-\u0026gt;mWriteCounter.store(mLocalCounter, std::memory_order_release); std::memcpy(mNextElement, \u0026amp;size, sizeof(int32_t)); std::memcpy(mNextElement + sizeof(int32_t), buffer.data(), buffer.size()); mQ-\u0026gt;mReadCounter.store(mLocalCounter, std::memory_order_release); mNextElement += payloadSize; } int32_t QConsumer::TryRead(std::span\u0026lt;std::byte\u0026gt; buffer) { if (mLocalCounter == mQ-\u0026gt;mWriteCounter.load(std::memory_order_acquire)) return 0; int32_t size; std::memcpy(\u0026amp;size, mNextElement, sizeof(int32_t)); int32_t writeCounter = mQ-\u0026gt;mWriteCounter.load(std::memory_order_acquire); EXPECT(writeCounter - mLocalWriter \u0026lt;= QUEUE_SIZE, \u0026#34;queue overflow\u0026#34;); EXPECT(size \u0026lt;= buffer.size(), \u0026#34;Buffer space isn\u0026#39;t large enough\u0026#34;); std::memcpy(buffer.data(), mNextElement + sizeof(size), size); const int32_t payloadSize = sizeof(size) + size; mLocalCounter += payloadSize; mNextElement += payloadSize; writeCounter = mQ-\u0026gt;mWriteCounter.load(std::memory_order_acquire); EXPECT(writeCounter - mLocalCounter \u0026lt;= QUEUE_SIZE, \u0026#34;queue overflow\u0026#34;); } Optimization #1: Caching the Write Counter 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 void QProducer::Write(std::span\u0026lt;std::byte\u0026gt; buffer) { const int32_t payloadSize = sizeof(int32_t) + buffer.size(); mLocalCounter += payloadSize; // we \u0026#34;reserve\u0026#34; more space (X% of the total queue) // to avoid touching this cache line on every message written if (mCachedWriteCounter \u0026lt; mLocalCounter) { mCachedWriteCounter = Align\u0026lt;Q_WRITE_COUNTER_BLOCK_BYTES\u0026gt;(mLocalCounter); mQ-\u0026gt;mWriteCounter.store(mCachedWriteCounter, std::memory_order_release); } std::memcpy(mNextElement, \u0026amp;payloadSize, sizeof(int32_t)); std::memcpy(mNextElement + sizeof(int32_t), buffer.data(), buffer.size()); mQ-\u0026gt;mReadCounter.store(mLocalCounter, std::memory_order_release); mNextElement += payloadSize; } Optimization #2: Data Alignment Optimization #3: Caching the Read Counter Going further - Zero copy Measurements in low-latency trading systems 1 2 3 4 5 6 7 8 9 10 void Executor::PollMarketData() { while (poll[\u0026amp;](const auto\u0026amp; msg)) { if (IsInteresting(msg)) { SendOrder(); } } } Clang Xray Instrumentation Principle #8: \u0026ldquo;Being fast is good - staying in fast is better\u0026rdquo;\nPrinciple #9: \u0026ldquo;Thinking abut the system as a whole\u0026rdquo;\nPrinciple #10: \u0026ldquo;The performance of your code depends on your colleagues\u0026rsquo; code as much as yours\u0026rdquo;\n","date":"2025-11-09T23:50:12+08:00","permalink":"https://sususu5.github.io/p/when-nanoseconds-matter-ultrafast-trading-systems-in-c/","title":"When Nanoseconds Matter: Ultrafast Trading Systems in C++"},{"content":"Adapting HPX\u0026rsquo;s Parallel Algorithms for Usage with Senders and Receivers What is partitioner_with_cleanup in HPX What are Senders and Receivers What is this project about Difficulties and challenges Completed works Future works Acknowledgments This report presents the final outcomes of my Google Summer of Code 2025 project with the Ste||ar Group and serves as complete documentation for anyone interested in the implementation details and results.\nHPX provides a comprehensive implementation of parallel algorithms that serves as both a standards-compliant C++ library and a high-performance computing runtime system. With the upcoming C++26 standard introducing the Senders/Receivers (S/R) programming model for asynchronous execution, it becomes crucial to ensure HPX\u0026rsquo;s compatibility with it.\nHowever, parallel algorithms based on the partitioner_with_cleanup component, which is used for task partitioning and resource cleanup, currently don\u0026rsquo;t have support for the S/R model, this project aims to bridge the gap by adding modification to this component, adjusting these algorithms themselves and providing comprehensive unit tests.\nWhat is partitioner_with_cleanup in HPX The hpx::parallel::util::partitioner_with_cleanup component in HPX library serves as the central execution engine for HPX\u0026rsquo;s parallel algorithms, the functionality of it is mostly the same as hpx::parallel::util::partitioner, but introduces extra cleanup semantic. This critical infrastructure component handles the complex task of dividing work into parallel chunks, managing their execution across different policies, and ensuring robust error handling and resource cleanup.\nTo be specific, the running process of partitioner_with_cleanup can be divided into the following steps:\nBind policy/executor parameters. Build partition shape and bulk-launch chunk work (f1) on executor. Reduce results (f2) while applying error handling. On any failure, run cleanup on successful chunks to roll back. Return result type dictated by policy Within the partitioner_with_cleanup component, HPX provides two distinct implementations to handle different execution models. The static_partitioner_with_cleanup is used for non-task execution policies, including sequenced/parallel policies and S/R execution. In contrast, the task_static_partitioner_with_cleanup is used for task-based execution policies such as sequenced_task and parallel_task.\nIn the call chain, there are two main functions, which are the call function and reduce function, they are responsible for the partitioning and reducing process respectively.\nThe call function takes the following parameters: an execution policy policy, a range (a forward iterator first and a std::size_t value count), a per-chunk worker f1, a final reducer f2, and a cleanup function cleanup. This function firstly partitions the range into chunks according to the execution policy, calling f1 on all chunks, then call the reduce function to merge the algorithm results.\n1 2 3 4 template \u0026lt;typename ExPolicy_, typename FwdIter, typename F1, typename F2, typename Cleanup\u0026gt; static decltype(auto) call(ExPolicy_\u0026amp;\u0026amp; policy, FwdIter first, std::size_t count, F1\u0026amp;\u0026amp; f1, F2\u0026amp;\u0026amp; f2, Cleanup\u0026amp;\u0026amp; cleanup) The reduce function takes three parameters: a list of chunk results workitems, a reducing function f, and a cleanup function cleanup, This function firstly waits for all workitems to complete. If there is any exception returned by workitems, the cleanup function will be called on all successful results and the captured exceptions are thrown again. If all the workitems are successful, a result of calling the reduce function will be returned.\n1 2 template \u0026lt;typename Items, typename F, typename Cleanup\u0026gt; static decltype(auto) reduce(Items\u0026amp;\u0026amp; workitems, F\u0026amp;\u0026amp; f, Cleanup\u0026amp;\u0026amp; cleanup) The following is an image that illustrates its workflow. If you are interested in more details about HPX\u0026rsquo;s parallel algorithms, a good reference is Tobias Wukovitsch\u0026rsquo;s project of Google Summer of Code 2024, which includes a clear and detailed introduction.\nWhat are Senders and Receivers Since this project is about S/R, it\u0026rsquo;s important to provide a brief introduction to it.\nModern C++ has long lacked a standard way to express asynchronous operations. Existing approaches such as futures, callbacks, or third-party libraries often suffer from inconsistent semantics or limited support for cancellation and error handling, which may cause potential issues.\nHence, the C++ standards committee has been working on a proposal known as P2300: std::execution, which introduces a new abstraction for asynchronous programming called the S/R model. This proposal is expected to form the foundation of future concurrency support in the C++ standard.\nAt its core, the S/R model separates two roles: the Sender, which produces an asynchronous result, and the Receiver, which consumes it. A sender does not run work immediately but describes what will be done once execution starts.\nTo execute a sender, it should be firstly connected to a receiver through connect(), which forms an operation state, this object knows what to do and where the result should be given to. Then, start() must be called on it to trigger the actual asynchronous execution.\nWhen the work completes, the sender signals the receiver in one of three ways:\nValue → the operation completed successfully and produced a result Error → the operation failed and communicates the failure downstream Stopped → the operation was cancelled before producing a result This tri-state completion model makes error handling and cancellation first-class citizens rather than afterthoughts. It also allows asynchronous operations to be composed in a consistent and declarative style.\nIn the exact codebase, developers often uses functions like sync_wait(), when_all(), and let_value() to implicitly complete the connect() and start() operations. Here is a simple example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #include \u0026lt;stdexec/execution.hpp\u0026gt; #include \u0026lt;iostream\u0026gt; int main() { // Create a thread pool and get its scheduler stdexec::static_thread_pool pool{2}; auto scheduler = pool.get_scheduler(); auto s = stdexec::just(42) // just() describes \u0026#34;generate 42\u0026#34; | stdexec::continue_on(scheduler) // continue_on() switches execution to the pool\u0026#39;s scheduler | stdexec::then([](int x){ return x * 2; }); // then() describes \u0026#34;times x by 2\u0026#34; // sync_wait() explicitly connects a receiver and starts it auto [result] = stdexec::sync_wait(s).value(); std::cout \u0026lt;\u0026lt; \u0026#34;Result = \u0026#34; \u0026lt;\u0026lt; result \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; // prints: 84 } What is this project about The primary goal of this project is to extend HPX\u0026rsquo;s parallel algorithms so that they fully support the S/R model when built on top of the partitioner_with_cleanup component. The work is organized into three parts:\nExtending partitioner_with_cleanup\nIntroduce logic branches to handle calls originating from S/R-based execution, covering both launch policies and execution policies. These adaptations ensure clean integration with the S/R model while preserving existing semantics.\nAdapting algorithm implementations\nModify affected algorithms to align with S/R usage. To be specific, changing the return type from algorithm_result to decltype(auto) so that senders are correctly deduced as return values. Additionally, disable early exits in the S/R branch to keep sender chains intact and faithfully represent the asynchronous workflow.\nDeveloping unit tests\nProvide a comprehensive suite of tests for the affected algorithms. Each test evaluates four combinations of launch and execution policies:\nhpx::launch::sync with seq(task) hpx::launch::sync with unseq(task) hpx::launch::async with seq(task) hpx::launch::async with unseq(task) These combinations cover major usage scenarios and mirror test patterns used elsewhere in HPX, ensuring consistency and reliability.\nDifficulties and challenges The key challenge of this project is how to modify the partitioner_with_cleanup component so that it can handle failures gracefully. While the regular partitioner component can assume everything will work perfectly and simply return the reduced results, partitioner_with_cleanup must be more cautious. It needs to track which subtasks succeed or fail, and more importantly, it must call cleanup functions for successful work when other parts of the operation fail.\nAs mentioned above, an asynchronous operation executed in the S/R model can finish in three distinct ways, which are Value, Error, and Stopped. Therefore, if any chunk fails, the partitioning function call completes on the error channel. Consequently, the reduce function can only observe an error signal rather than all the chunk results from the algorithm, making it impossible to identify which chunks completed successfully and preventing proper cleanup function from being called.\nTherefore, we need to distinguish between normal results and errors during execution. To achieve this, the call function wraps the original algorithm parameter f1 in a lambda expression, ensuring that every return value is stored in a variant type. Each partition task therefore produces either a normal Result or an exception_ptr.\nNext, the reduce function linearly traverses all partition results. If it detects an error, it first releases the resources held by already successful subtasks, and then rethrows the exception.\nThis pseudocode illustrates the principle, actual implementation differs in template details.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // Here is a simplified version of the above idea // 1. Wrap f1 so each chunk returns either Result or exception_ptr auto wrapped_f1 = [\u0026amp;](auto\u0026amp;\u0026amp;... args) -\u0026gt; std::variant\u0026lt;Result, std::exception_ptr\u0026gt; { try { return f1(std::forward\u0026lt;decltype(args)\u0026gt;(args)...); } catch (...) { return std::current_exception(); } }; // 2. Partition returns a collection of variant results auto items = partition\u0026lt;std::variant\u0026lt;Result, std::exception_ptr\u0026gt;\u0026gt;( policy, first, count, wrapped_f1); // 3. Reduce traverses all items, cleans up successes, rethrows on error for (auto\u0026amp; item : items) { if (std::holds_alternative\u0026lt;std::exception_ptr\u0026gt;(item)) { cleanup_successful(items); std::rethrow_exception(std::get\u0026lt;std::exception_ptr\u0026gt;(item)); } } In this way, although each subtask passes its result by value, the final Sender always completes in a successful state by returning a container of variants. The caller can then inspect this container to uniformly detect errors, perform cleanup, and rethrow exceptions if necessary.\nCompleted works This project successfully integrated S/R model support into all parallel algorithms that utilize the partitioner_with_cleanup component, accompanied by comprehensive unit test coverage.\nMemory Construction Algorithms:\nuninitialized_copy, uninitialized_copy_n uninitialized_default_construct, uninitialized_default_construct_n uninitialized_fill, uninitialized_fill_n uninitialized_value_construct, uninitialized_value_construct_n Memory Movement Algorithms:\nuninitialized_move, uninitialized_move_n uninitialized_relocate, uninitialized_relocate_backward, uninitialized_relocate_n All implementation details, code modifications, and test cases can be reviewed in PR #6741, which contains the complete set of changes of project.\nFuture Work This project implements a solid extension for S/R model integration in HPX, with the following development priorities:\nThe immediate focus should be completing algorithm coverage by adapting remaining parallel algorithms to the S/R model. Current progress and remaining work items are tracked in this table.\nNext, development should shift toward performance optimization through comprehensive benchmarking and advanced S/R composition patterns to minimize overhead and maximize efficiency.\nAcknowledgments I would like to express my sincere gratitude to Hartmut Kaiser, Isidoros Tsaousis-Seiras and Panos Syskakis for their invaluable guidance and continuous support throughout this project. Their expertise was instrumental in helping me navigate complex technical challenges.\nI\u0026rsquo;m also grateful to Tobias Wukovitsch for his excellent work and report in Google Summer of Code 2024, which provided essential background for this work.\nFinally, I thank Google and the Google Summer of Code program for providing this incredible opportunity to contribute to open-source software and grow as a developer.\n","date":"2025-08-29T18:23:46+10:00","permalink":"https://sususu5.github.io/p/google-summer-of-code-final-report/","title":"Google Summer of Code Final Report"},{"content":"How to Implement a C++ Vector From Scratch Introduction std::vector is a sequence container in C++ that represents a dynamic array. It is part of the C++ Standard Template Library (STL) and provides the ability to store and manipulate a collection of elements with automatic memory management and flexible resizing.\nstd::vector is the most commonly used container in C++ development, whether you are passionate about competitive coding or interested in C++ programming, learning the low-level implementation of it helps you gain a more thorough understanding of C++ language and prepares you for better using it in the future.\nPrerequisite Knowledge Rule of Five The Rule of Five in C++ is a guideline helping developers manage resource ownership in user-defined types.\nTo be specific, if your class manages a resource, and you define any one of the following five special member functions, you should probably define all five to ensure correct behavior:\nDestructor Copy Constructor Copy Assignment Operator Move Constructor Move Assignment Operator reinterpret_cast\u0026lt;T\u0026gt;(expression) reinterpret_cast is a low-level type conversion operator in C++ used to reinterpret the binary representation of a value as a different type.\nTwo important thing about it is that it does not perform any type safety checks and it simply treats the bits of one type as if they were another.\n::operator new and ::operator delete ::operator new is a built-in global function in C++ that allocates raw, uninitialized memory on the heap.\nIt is not the same as the new expression, though the new expression uses it internally. The :: scope resolution prefix ensures you’re calling the global version of operator new, avoiding any class-specific overloads.\n::operator delete is the global deallocation function to free raw memory that was previously allocated using ::operator new.\nStep by Step Implementation Private Fields To implement a std::vector, firstly we need to define the private fields of this class.\n1 2 3 T* data_ = nullptr; // data_ is a pointer pointing to a generic type T size_t capacity_ = 0; size_t size_ = 0; Additionally, we need to define the initial size of a vector and the growth factor. The value of a growth factor is the growing rate of a vector once the capacity is full.\n1 2 static constexpr size_t startSize_ = 10; static constexpr double growthFactor_ = 1.6; // if growthFactor = 2, the vector will be doubled when it\u0026#39;s full Constructors and Destructor According to Rule of Five, we need to implement three types of constructors for out vector class, which are Default Constructor, Copy Constructor, and Move Constructor.\nIn Default Constructor, we need to allocate initial capacity memory for the underlying data and initialzie the member variables.\n1 2 3 4 Vector() { data_ = reinterpret_cast\u0026lt;T*\u0026gt;(::operator new(startSize_ * sizeof(T))); capacity_ = startSize_; } In Copy Constructor, we need to allocate capacity for a new vector and keep the input object unchanged.\n1 2 3 4 5 6 Vector(const Vector\u0026amp; other) : size_(other.size_), capacity_(other.capacity_) { data_ = reinterpret_cast\u0026lt;T*\u0026gt;(::operator new(capacity_ * sizeof(T))); for (size_t i = 0; i \u0026lt; size_; ++i) { new (data_ + i) T(other.data_[i]); } } In Move Constructor, we need to transfer the ownership of resources from one object to another instead of copying, which means the input object of this constructor should be set to void.\n1 2 3 4 5 6 Vector(Vector\u0026amp;\u0026amp; other) noexcept : data_(other.data_), capacity_(other.capacity_), size_(other.size_) { other.data_ = nullptr; other.capacity_ = 0; other.size_ = 0; } In the Destructor\n1 2 3 4 5 6 ~Vector() { for (size_t i = 0; i \u0026lt; size_; ++i) { data_[i].~T(); } ::operator delete(data_); } Assignments According to the Rule of Five, both the copy assignemnt and the move assignemnt must be implemented for this class.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // Copy assignment Vector\u0026amp; operator=(const Vector\u0026amp; other) { if (this != \u0026amp;other) { Vector temp(other); swap(temp); } return *this; } // Move assignment Vector\u0026amp; operator=(Vector\u0026amp;\u0026amp; other) noexcept { if (this != \u0026amp;other) { Vector temp(std::move(other)); swap(temp); } return *this; } Out of convenience, we can also define a specific swap function, which swaps another object with this.\n1 2 3 4 5 void swap(Vector\u0026amp; other) noexcept { std::swap(data_, other.data_); std::swap(capacity_, other.capacity_); std::swap(size_, other.size_); } Element Insertion In std::vector, the push_back method appends a new element to the end of the container and there are two versions should be implemented, which are the one handling the left value\n1 2 3 4 5 6 7 8 9 10 void push_back(const T\u0026amp; elem) { if (size_ == capacity_) { reserve(capacity_ ? static_cast\u0026lt;size_t\u0026gt;(capacity_ * growthFactor_) : startSize_); assert(size_ \u0026lt; capacity_); } assert(data_); new (data_ + size_) T(elem); ++size_; } and the one handling the right value.\n1 2 3 4 5 6 7 8 9 10 void push_back(T\u0026amp;\u0026amp; elem) { if (size_ == capacity_) { reserve(capacity_ ? static_cast\u0026lt;size_t\u0026gt;(capacity_ * growthFactor_) : startSize_); assert(size_ \u0026lt; capacity_); } assert(data_); new (data_ + size_) T(std::move(elem)); ++size_; } Element Access To allow users to access elements by index, we need to overload two versions of operator[], which handle both const and non-const objects.\n1 2 3 4 5 6 7 8 9 const T\u0026amp; operator[](size_t idx) const { assert(idx \u0026lt; size_); return data_[idx]; } T\u0026amp; operator[](size_t idx) { assert(idx \u0026lt; size_); return data_[idx]; } In addition, common operations like front, back, and size should also be added, which are\n1 2 3 4 5 6 7 8 9 10 11 12 13 const T\u0026amp; front() const { assert(size_ \u0026gt; 0); return operator[](0); } const T\u0026amp; back() const { assert(size_ \u0026gt; 0); return operator[](size_ - 1); } size_t size() const { return size_; } size_t capacity() const { return capacity_; } bool empty() const { return size_ == 0; } Element Removal \u0026amp; Container Modification There are three operations to remove elemnt from a vector, which are pop_back, erase, and clear.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 // Remove element at specific index void erase(size_t idx) { if (idx \u0026gt;= size_) return; // Destroy the element at idx data_[idx].~T(); // Move elements to fill the gap for (size_t j = idx; j \u0026lt; size_ - 1; ++j) { new (data_ + j) T(std::move(data_[j + 1])); data_[j + 1].~T(); } --size_; } // Remove all elements in the vector void clear() { for (size_t i = 0; i \u0026lt; size_; ++i) { data_[i].~T(); } size_ = 0; } // Remove an element at the back of this vector void pop_back() { if (size_ \u0026gt; 0) { --size_; data_[size_].~T(); } } Memory Management The reserve operation is used to pre-allocate memory for a container, which is useful when you know how many elements will be added to a vector.\n1 2 3 4 5 6 7 8 9 10 11 12 void reserve(size_t newCapacity) { if (capacity_ \u0026gt;= newCapacity) return; auto* newData = reinterpret_cast\u0026lt;T*\u0026gt;(::operator new(newCapacity * sizeof(T))); for (size_t i = 0; i \u0026lt; size_; ++i) { new (newData + i) T(std::move(data_[i])); data_[i].~T(); } ::operator delete(data_); data_ = newData; capacity_ = newCapacity; } Complete Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 #include \u0026lt;cstddef\u0026gt; #include \u0026lt;cassert\u0026gt; #include \u0026lt;cstdlib\u0026gt; #include \u0026lt;utility\u0026gt; #include \u0026lt;new\u0026gt; #include \u0026lt;vector\u0026gt; template \u0026lt;typename T\u0026gt; class Vector { public: // Default constructor Vector() { data_ = reinterpret_cast\u0026lt;T*\u0026gt;(::operator new(startSize_ * sizeof(T))); capacity_ = startSize_; } // Copy constructor Vector(const Vector\u0026amp; other) : size_(other.size_), capacity_(other.capacity_) { data_ = reinterpret_cast\u0026lt;T*\u0026gt;(::operator new(capacity_ * sizeof(T))); for (size_t i = 0; i \u0026lt; size_; ++i) { new (data_ + i) T(other.data_[i]); } } // Move constructor Vector(Vector\u0026amp;\u0026amp; other) noexcept : data_(other.data_), capacity_(other.capacity_), size_(other.size_) { other.data_ = nullptr; other.capacity_ = 0; other.size_ = 0; } // Copy assignment Vector\u0026amp; operator=(const Vector\u0026amp; other) { if (this != \u0026amp;other) { Vector temp(other); swap(temp); } return *this; } // Move assignment Vector\u0026amp; operator=(Vector\u0026amp;\u0026amp; other) noexcept { if (this != \u0026amp;other) { Vector temp(std::move(other)); swap(temp); } return *this; } // Destructor ~Vector() { for (size_t i = 0; i \u0026lt; size_; ++i) { data_[i].~T(); } ::operator delete(data_); } // Left value reference void push_back(const T\u0026amp; elem) { if (size_ == capacity_) { reserve(capacity_ ? static_cast\u0026lt;size_t\u0026gt;(capacity_ * growthFactor_) : startSize_); assert(size_ \u0026lt; capacity_); } assert(data_); new (data_ + size_) T(elem); // Fixed: construct at correct position with elem ++size_; } // Right value reference void push_back(T\u0026amp;\u0026amp; elem) { if (size_ == capacity_) { reserve(capacity_ ? static_cast\u0026lt;size_t\u0026gt;(capacity_ * growthFactor_) : startSize_); assert(size_ \u0026lt; capacity_); } assert(data_); new (data_ + size_) T(std::move(elem)); ++size_; } // Const value overload const T\u0026amp; operator[](size_t idx) const { assert(idx \u0026lt; size_); return data_[idx]; } // Non-const value overload T\u0026amp; operator[](size_t idx) { assert(idx \u0026lt; size_); return data_[idx]; } const T\u0026amp; front() const { assert(size_ \u0026gt; 0); return operator[](0); } const T\u0026amp; back() const { assert(size_ \u0026gt; 0); return operator[](size_ - 1); } size_t size() const { return size_; } size_t capacity() const { return capacity_; } bool empty() const { return size_ == 0; } void reserve(size_t newCapacity) { if (capacity_ \u0026gt;= newCapacity) return; auto* newData = reinterpret_cast\u0026lt;T*\u0026gt;(::operator new(newCapacity * sizeof(T))); for (size_t i = 0; i \u0026lt; size_; ++i) { new (newData + i) T(std::move(data_[i])); data_[i].~T(); } ::operator delete(data_); data_ = newData; capacity_ = newCapacity; } void erase(size_t idx) { if (idx \u0026gt;= size_) return; // Destroy the element at idx data_[idx].~T(); // Move elements to fill the gap for (size_t j = idx; j \u0026lt; size_ - 1; ++j) { new (data_ + j) T(std::move(data_[j + 1])); data_[j + 1].~T(); } --size_; } void clear() { for (size_t i = 0; i \u0026lt; size_; ++i) { data_[i].~T(); } size_ = 0; } void pop_back() { if (size_ \u0026gt; 0) { --size_; data_[size_].~T(); } } private: void swap(Vector\u0026amp; other) noexcept { std::swap(data_, other.data_); std::swap(capacity_, other.capacity_); std::swap(size_, other.size_); } T* data_ = nullptr; size_t capacity_ = 0; size_t size_ = 0; static constexpr size_t startSize_ = 10; static constexpr double growthFactor_ = 1.6; }; int main() { Vector\u0026lt;int\u0026gt; v; assert(v.size() == 0); for (int i = 0; i \u0026lt; 100; ++i) v.push_back(i); for (int i = 0; i \u0026lt; 100; ++i) assert(v[i] == i); assert(v.size() == 100); for (int i = 0; i \u0026lt; 50; ++i) { assert(v[0] == i); v.erase(0); } assert(v.size() == 50); for (int i = 50; i \u0026lt; 100; ++i) { assert(v[0] == i); v.erase(0); } assert(v.size() == 0); Vector\u0026lt;int\u0026gt; v2; v2.push_back(42); assert(v2.front() == 42); assert(v2.back() == 42); Vector\u0026lt;int\u0026gt; v3 = v2; assert(v3.size() == 1); assert(v3[0] == 42); return 0; } ","date":"2025-06-11T13:51:00+10:00","permalink":"https://sususu5.github.io/p/how-to-implement-a-c-vector-from-scratch/","title":"How to Implement a C++ Vector From Scratch"},{"content":"How to design a POI (Point of Interest) Service Functional Requirements Searching nearby locations Viewing details Owners can edit / delete Non-functional Requirements Latency: 1s Freshness: 1 day -\u0026gt; 1 hour Scalability Fault Tolerance Assumptions 1B users -\u0026gt; 50% -\u0026gt; 500M DAU 200M businesses Storage: NoSQL, SQL, in memory Database Selection In-memory: using multiple machines to shard the data, and the latency requirement is 1s, it\u0026rsquo;s expensive to use this SQL: more expensive NoSQL: cheaper, fast querying, self-sharding and optimization, suitable How to scale searching Master-Slave Model, separate reading and writing This system is suitable for using this model because the need for reading is much more than writing Not using whole table scaning here, given longtitude and latitude Composite Index can not speed up the searching here because it follows the Leftmost Prefix Rule, we can only search the range of the latitude once the longitude is matched, so it equals whole table scaning Two other effective searching ways will be introduced in the following content GeoHash GeoHash is a spatial hashing algorithm that continuously divides latitude and longitude ranges in half (using a Z-order curve), converting a (latitude, longitude) pair into a Base32 string that represents the geographic grid cell in which the location falls It is a value stored in the database The longer the string, the higher the precision of the geographical location Quad Tree A Quad Tree is a tree data structure that recursively partitions a two-dimensional space into four quadrants (subregions). Each node in the tree has up to four children representing the top-left, top-right, bottom-left, and bottom-right regions It is a memory structure Quad Tree can be used as indicies to speed up the searching (k values), assume each node contains about 100 businesses Can it fits into memory? (business_id, latitude, longitute, short_brief) \u0026lt; 50B (200M / 100) * (100 * 50B) = 10GB, which can be stored in memory, meets the latency requirement in 1 second\nHow to build this tree? Need a starting point, stored on disk as a seed value\nHow to update this tree? Live update / eventually consistecy\nAdding a cache to store the latitude, longitude, and their corresponding business list, extra cost.\nConstruct the Quad Tree according to the deployment (west, middle, east USA), extra cost, availability\n","date":"2025-06-07T11:43:38+10:00","permalink":"https://sususu5.github.io/p/how-to-design-a-poi-point-of-interest-service/","title":"How to Design a POI (Point of Interest) Service"},{"content":"How to design a TikTok Functional Requirements Video Upload: Allow users to upload videos to the platform. Video Playback: Enable users to watch videos. By developing these, we can construct a minimum viable product.\nNon-functional Requirements Scalability: Support a massive user base with high concurrency. Availability: Ensure the system remains operational even during partial failures. Low Latency: Minimize delays in video loading and playback. Fault Tolerance: Handle hardware/network failures gracefully without data loss. Assumptions User Base: 1 billion daily active users (DAU). Usage Patterns: Each user watches 100 videos per day. Each user uploads 1 video per day. Video Size: Average video size is 10MB. Database Selection: SQL vs NoSQL SQL Databases: Pros: Strong consistency, relational data support. Cons: Challenges with sharding and hotspot management. NoSQL Databases: Pros: Cost-effective, horizontally scalable. Cons: Limited transactional support. Video Storage Strategy Blob Storage (Binary Large Object):\nOptimized for unstructured data (videos, images, audio). Ideal for storing and retrieving large volumes of small files efficiently. How to upload a video Since we don\u0026rsquo;t know what users are uploading, exposing the storage to the interface directly is unsafe. A better option is allocating a temporary space to store the original videos uploaded by users. When uploading, a video can be cut into small pieces to support breakpoint resume upload when a break happens, and also parallel uploading, which means multiple segments can be uploaded simultaneously. (for a mobile app, the network environment is not stable) Once all segments are uploaded, we can use a message queue and a worker pool to merge all the segments and do a file integrity verification. After that, this video should be encoded into different formats because videos of different qualities should be played according to devices and network. How to watch a video To avoid hotspots caused by frequent access to popular videos, we can deploy a CDN near user locations to offload traffic from the blob storage. Although storing videos in a CDN speeds up delivery and reduces latency, it also comes with high costs. So, we should make sure only the most popular videos are cached there. By introducing an extractor service, it can regularly find popular videos from the blob storage and send them to the CDN, those outdated videos are replaced. We can also introduce a streaming protocol like the HTTP Live Streaming from Apple to realize \u0026ldquo;stream-as-you-go\u0026rdquo; manner, which improves user experience. Show off We can introduce a recommendation system to recommend videos to users rather than pushing original feed accoring to the time. We can introduce a Two-Tower Model to embed user and video features into separate vectors. When a client requests videos, the system can recommend those that match their interests based on vector similarity. The pro is the vieo watching time of clients can be extended, the con is the extra cost of hiring a team to construct and deploy the model. ","date":"2025-06-03T16:18:56+10:00","permalink":"https://sususu5.github.io/p/how-to-design-a-tiktok/","title":"How to Design a TikTok"}]