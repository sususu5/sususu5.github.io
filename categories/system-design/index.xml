<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>System Design on Zhengnan Hua</title>
        <link>https://sususu5.github.io/categories/system-design/</link>
        <description>Recent content in System Design on Zhengnan Hua</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <copyright>Zhengnan Hua</copyright>
        <lastBuildDate>Wed, 07 Jan 2026 15:31:14 +0800</lastBuildDate><atom:link href="https://sususu5.github.io/categories/system-design/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>How to Design a Ticket Selling System</title>
        <link>https://sususu5.github.io/p/how-to-design-a-ticket-selling-system/</link>
        <pubDate>Wed, 07 Jan 2026 15:31:14 +0800</pubDate>
        
        <guid>https://sususu5.github.io/p/how-to-design-a-ticket-selling-system/</guid>
        <description>&lt;h1 id=&#34;how-to-design-a-ticket-selling-system&#34;&gt;How to Design a Ticket Selling System
&lt;/h1&gt;&lt;h2 id=&#34;functional-requirements&#34;&gt;Functional Requirements
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Purchase Tickets:&lt;/strong&gt; Users can buy tickets.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Refund Tickets:&lt;/strong&gt; Users can withdraw/refund tickets.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Select Time:&lt;/strong&gt; Users can choose the event time.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Payment Processing:&lt;/strong&gt; Handle user payments securely.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Real-time Availability:&lt;/strong&gt; Users should see the number of remaining tickets.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Seat Selection:&lt;/strong&gt; Users can choose their specific seats.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Two-Phase Purchasing:&lt;/strong&gt; Ticket buying is split into reservation and payment. If payment isn&amp;rsquo;t completed within a pre-defined window (e.g., 15 minutes), the reservation is released.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;non-functional-requirements&#34;&gt;Non-functional Requirements
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;High Concurrency:&lt;/strong&gt; The system must handle a massive spike in read and write traffic immediately after sales open.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;High Availability:&lt;/strong&gt; The system must remain responsive and usable during peak traffic.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Consistency:&lt;/strong&gt; Ensure strong consistency for inventory to prevent over-selling.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;back-of-the-envelope-calculation&#34;&gt;Back-of-the-envelope Calculation
&lt;/h2&gt;&lt;p&gt;&lt;em&gt;Assumptions:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Total tickets: 100,000&lt;/li&gt;
&lt;li&gt;Active users checking tickets: 5 million&lt;/li&gt;
&lt;li&gt;Time window: 1 minute (traffic spike upon opening)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Read QPS:&lt;/em&gt;
If requests are evenly distributed over 1 minute:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;5,000,000 req / 60 sec â‰ˆ 83,333 req/sec&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;(Rounded to ~80,000 QPS)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Write QPS:&lt;/em&gt;
Assuming 80% of active users attempt to buy:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;80,000 req/sec * 0.8 = 64,000 req/sec&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;Peak Traffic:&lt;/em&gt;
In the first few seconds, traffic could be 5-10x the average, requiring the system to handle significant burst loads.&lt;/p&gt;
&lt;h2 id=&#34;high-level-design&#34;&gt;High-level Design
&lt;/h2&gt;&lt;p&gt;&lt;img src=&#34;https://sususu5.github.io/p/how-to-design-a-ticket-selling-system/design1.png&#34;
	width=&#34;2094&#34;
	height=&#34;1126&#34;
	srcset=&#34;https://sususu5.github.io/p/how-to-design-a-ticket-selling-system/design1_hu_124f1937efdd26a8.png 480w, https://sususu5.github.io/p/how-to-design-a-ticket-selling-system/design1_hu_497bd607a8d194a8.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;High Level Design&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;185&#34;
		data-flex-basis=&#34;446px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;design-deep-dive&#34;&gt;Design Deep Dive
&lt;/h2&gt;&lt;h3 id=&#34;1-inventory-deduction&#34;&gt;1. Inventory Deduction
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Challenge:&lt;/strong&gt; With only 1 ticket left and 1,000 requests flooding the queue, how do we efficiently block the 999 failing requests and implement &amp;ldquo;inventory pre-deduction&amp;rdquo;?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;
When the writing service accepts a request, the pre-deduction should occur in a Redis cache and be queued in the Message Queue (MQ) simultaneously.
If an order is cancelled, the inventory must be restored in Redis, and the MQ should be notified to ignore any processing related to that specific cancelled transaction.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://sususu5.github.io/p/how-to-design-a-ticket-selling-system/design2.png&#34;
	width=&#34;2122&#34;
	height=&#34;1162&#34;
	srcset=&#34;https://sususu5.github.io/p/how-to-design-a-ticket-selling-system/design2_hu_4f2fa56f70a3ea13.png 480w, https://sususu5.github.io/p/how-to-design-a-ticket-selling-system/design2_hu_1fd72abdbf3f19b4.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Detailed Design&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;182&#34;
		data-flex-basis=&#34;438px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;2-the-atomicity-challenge&#34;&gt;2. The Atomicity Challenge
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Challenge:&lt;/strong&gt; Redis &lt;code&gt;GET&lt;/code&gt; and &lt;code&gt;DECR&lt;/code&gt; operations are not atomic by default. If two users request the last ticket simultaneously, a race condition could lead to over-selling.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;
Use &lt;strong&gt;Lua scripts&lt;/strong&gt; in Redis. Lua scripts are executed atomically; even if multiple requests arrive at the same time, Redis executes the script sequentially, ensuring the check-and-decrement logic is safe.&lt;/p&gt;
&lt;h3 id=&#34;3-the-dual-write-problem&#34;&gt;3. The Dual-Write Problem
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Challenge:&lt;/strong&gt; If the service crashes or experiences network issues (network jitter) after updating Redis but before sending the message to the MQ, the inventory data becomes inconsistent (tickets &amp;ldquo;disappear&amp;rdquo;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;
Use &lt;strong&gt;Transactional Messages&lt;/strong&gt; (e.g., in RocketMQ). The flow is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Prepare:&lt;/strong&gt; The writing service sends a &amp;ldquo;half message&amp;rdquo; to the MQ (invisible to consumers).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Local Transaction:&lt;/strong&gt; Execute the Redis Lua script to deduct inventory.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Commit/Rollback:&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;If Redis deduction succeeds, send a &lt;strong&gt;Commit&lt;/strong&gt; to the MQ (making the message visible).&lt;/li&gt;
&lt;li&gt;If it fails, send a &lt;strong&gt;Rollback&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reconciliation:&lt;/strong&gt; If the service crashes before step 3, the MQ broker periodically queries the service to check the status of the local transaction and decide whether to commit or roll back.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;4-the-15-minute-release-strategy&#34;&gt;4. The 15-Minute Release Strategy
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Challenge:&lt;/strong&gt; How to release inventory for orders that remain unpaid after 15 minutes?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;
Leverage &lt;strong&gt;Delay Queues&lt;/strong&gt; (available in RocketMQ or via plugins in RabbitMQ).&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;When an order is created, the service sends a message with a 15-minute delay to the MQ.&lt;/li&gt;
&lt;li&gt;After 15 minutes, the consumer receives the message.&lt;/li&gt;
&lt;li&gt;The consumer checks the database for the order&amp;rsquo;s payment status. If unpaid, it cancels the order and restores the inventory.&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>How to Design a Chat System</title>
        <link>https://sususu5.github.io/p/how-to-design-a-chat-system/</link>
        <pubDate>Wed, 24 Dec 2025 16:38:41 +0800</pubDate>
        
        <guid>https://sususu5.github.io/p/how-to-design-a-chat-system/</guid>
        <description>&lt;h1 id=&#34;how-to-design-a-chat-system-at-scale&#34;&gt;How to Design a Chat System at Scale
&lt;/h1&gt;&lt;h2 id=&#34;requirements&#34;&gt;Requirements
&lt;/h2&gt;&lt;h3 id=&#34;functional-requirements&#34;&gt;Functional Requirements
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;One-to-One Chat&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;Real-time online message delivery.&lt;/li&gt;
&lt;li&gt;Reliable offline message storage and retrieval.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Group Chat&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;Support for small to medium groups (&amp;lt; 200 members).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Presence Status&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;Real-time indicators for Online/Offline/Busy states.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;non-functional-requirements&#34;&gt;Non-functional Requirements
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: Support &lt;strong&gt;1 Billion&lt;/strong&gt; registered users.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Low Latency&lt;/strong&gt;: End-to-end latency &lt;strong&gt;&amp;lt; 500ms&lt;/strong&gt; (aiming for near real-time).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;High Consistency&lt;/strong&gt;: Messages must be delivered in order (causal consistency) and exactly once.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fault Tolerance&lt;/strong&gt;: The system must be resilient to server failures without data loss.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;communication-protocols-message-model&#34;&gt;Communication Protocols (Message Model)
&lt;/h2&gt;&lt;p&gt;To achieve low latency and bi-directional communication, we compare the following mechanisms:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Polling&lt;/strong&gt;: Client periodically requests updates. &lt;em&gt;Inefficient due to high overhead and latency.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Long Polling&lt;/strong&gt;: Client holds connection until data is available. &lt;em&gt;Better than polling, but still resource-heavy on server headers/handshakes.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;WebSocket (Recommended)&lt;/strong&gt;: Persistent, full-duplex TCP connection. Ideal for real-time messaging.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;QUIC / HTTP/3&lt;/strong&gt;: UDP-based transport with congestion control. &lt;em&gt;Excellent for unstable networks (mobile clients) and reducing head-of-line blocking.&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Decision&lt;/strong&gt;: &lt;strong&gt;WebSocket&lt;/strong&gt; is the industry standard for the connection layer due to its widespread support and efficiency for stateful connections.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;architecture--scalability&#34;&gt;Architecture &amp;amp; Scalability
&lt;/h2&gt;&lt;h3 id=&#34;connection-management-session-affinity&#34;&gt;Connection Management (Session Affinity)
&lt;/h3&gt;&lt;p&gt;To ensure low latency, we need a stateful connection layer (Gateway/Chat Server). A user&amp;rsquo;s WebSocket connection must persist on a specific server.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Service Discovery &amp;amp; Session Storage:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We use a centralized Key-Value store (e.g., Redis) to map users to their physical servers.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Data Structure&lt;/strong&gt;: &lt;code&gt;&amp;lt;user_id, {last_heartbeat: timestamp, chat_server_id, ...}&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Workflow&lt;/strong&gt;: When User A wants to send a message to User B, the router looks up User B&amp;rsquo;s &lt;code&gt;chat_server_id&lt;/code&gt; in Redis to forward the payload.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Capacity Planning (Memory for Sessions):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Total Users: 1B&lt;/li&gt;
&lt;li&gt;Daily Active Users (DAU): 1B * 20% = 200M&lt;/li&gt;
&lt;li&gt;Peak Concurrent Users: 200M * 25% = 50M&lt;/li&gt;
&lt;li&gt;Memory Usage per User Session: approx 100 bytes (fd, ip_addr, port, state, user_id&amp;hellip;)&lt;/li&gt;
&lt;li&gt;Total Memory Required: 50M * 100 bytes = 5GB&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Note: 5GB fits easily into the RAM of a modern Redis cluster.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://sususu5.github.io/p/how-to-design-a-chat-system/session_affinity.png&#34;
	width=&#34;1542&#34;
	height=&#34;638&#34;
	srcset=&#34;https://sususu5.github.io/p/how-to-design-a-chat-system/session_affinity_hu_27f898c87af98d16.png 480w, https://sususu5.github.io/p/how-to-design-a-chat-system/session_affinity_hu_d07f71721aff593c.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Session Affinity&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;241&#34;
		data-flex-basis=&#34;580px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;message-synchronization--throughput&#34;&gt;Message Synchronization &amp;amp; Throughput
&lt;/h3&gt;&lt;p&gt;To decouple message ingestion from delivery and storage, we introduce a &lt;strong&gt;Message Queue&lt;/strong&gt; (e.g., Kafka) between the connection layer and the backend logic.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Throughput Calculation (Peak Time):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Average message rate: 0.2 msg/sec/user&lt;/li&gt;
&lt;li&gt;Total Throughput: 50M users * 0.2 = 10M msg/s&lt;/li&gt;
&lt;li&gt;Message Size:
&lt;ul&gt;
&lt;li&gt;Header (IDs): 8 * 3 = 24 bytes&lt;/li&gt;
&lt;li&gt;Content: 600-800 bytes&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Total (Compressed)&lt;/strong&gt;: approx 0.5KB&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Bandwidth: 10M msg/s * 0.5KB = 5GB/s&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Scaling Strategy:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Sharding&lt;/strong&gt;: Since 5GB/s is high for a single cluster, we must shard the Message Queue by &lt;code&gt;user_id&lt;/code&gt; (or &lt;code&gt;group_id&lt;/code&gt; for groups) to ensure ordering while distributing load.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Server Count&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;Benchmark: WhatsApp (Erlang-based) achieved 2M concurrent connections per server.&lt;/li&gt;
&lt;li&gt;Required Chat Servers: 50M / 2M = 25 Servers.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Result: This is highly manageable with modern C++ asynchronous I/O (epoll/io_uring).&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://sususu5.github.io/p/how-to-design-a-chat-system/message_sync.png&#34;
	width=&#34;1532&#34;
	height=&#34;1118&#34;
	srcset=&#34;https://sususu5.github.io/p/how-to-design-a-chat-system/message_sync_hu_940a89a74141f10d.png 480w, https://sususu5.github.io/p/how-to-design-a-chat-system/message_sync_hu_6211a90ae3ad6446.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Message Sync&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;137&#34;
		data-flex-basis=&#34;328px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;global-distribution-edge-aggregation&#34;&gt;Global Distribution (Edge Aggregation)
&lt;/h3&gt;&lt;p&gt;To handle the 5GB/s global traffic and reduce latency for cross-region users, we use &lt;strong&gt;Edge Aggregation&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Mechanism&lt;/strong&gt;: Users connect to the nearest Edge Node. Edge Nodes aggregate messages and send batch requests to the Central Data Center (or cross-region sync).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Edge Capacity&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;Assume one region handles 2M users.&lt;/li&gt;
&lt;li&gt;Traffic: 2M * 0.2 * 0.5KB = 200MB/s.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Feasibility&lt;/em&gt;: A single modern server with 10Gbps NIC can handle this comfortably.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://sususu5.github.io/p/how-to-design-a-chat-system/edge_aggr.png&#34;
	width=&#34;1538&#34;
	height=&#34;1134&#34;
	srcset=&#34;https://sususu5.github.io/p/how-to-design-a-chat-system/edge_aggr_hu_64c27bf035d1616b.png 480w, https://sususu5.github.io/p/how-to-design-a-chat-system/edge_aggr_hu_d2b5383cff5bd964.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Edge Aggr &amp;#43; Center Sync&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;135&#34;
		data-flex-basis=&#34;325px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;group-chat-design&#34;&gt;Group Chat Design
&lt;/h2&gt;&lt;p&gt;Group chats introduce a &amp;ldquo;Fan-out&amp;rdquo; problem (1 sender, N receivers).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data Model:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;group_id&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1001&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;message_id&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;987654321&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;sender_id&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;123&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;content&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Hello World&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;timestamp&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1700000000&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Routing Strategy:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Instead of broadcasting to all users immediately, we use a &lt;strong&gt;Group Register/Service&lt;/strong&gt;.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Group Registration&lt;/strong&gt;: Records which Chat Servers host &lt;em&gt;online&lt;/em&gt; members of a specific group.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Delivery&lt;/strong&gt;: The message is routed only to those specific Chat Servers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Optimization&lt;/strong&gt;: For large groups, limit the fan-out by only pushing notifications to active members, while others pull on demand.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Architectural Patterns:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Push Model / Write Fanout&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Suitable for small groups and real-time communication.&lt;/li&gt;
&lt;li&gt;Mechanism: When a user sends a message to a group, the message is pushed to all online members of the group.&lt;/li&gt;
&lt;li&gt;Pros: Fast read operations, users only need to read from their inbox.&lt;/li&gt;
&lt;li&gt;Cons: High write overhead, the more members in a group, the more messages need to be pushed.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pull Model / Read Fanout&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Suitable for large groups and offline messages.&lt;/li&gt;
&lt;li&gt;Mechanism: When a user sends a message to a group, the message is pushed to the group register/service, and the members will pull the messages from the group register/service on demand.&lt;/li&gt;
&lt;li&gt;Pros: Fast write operations, users only need to write to the group register/service.&lt;/li&gt;
&lt;li&gt;Cons: High read overhead, the more members in a group, the more messages need to be pulled.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;storage--offline-buffering&#34;&gt;Storage &amp;amp; Offline Buffering
&lt;/h2&gt;&lt;p&gt;We need a tiered storage strategy: fast write for recent messages and cost-effective storage for history.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Capacity Planning (Storage):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Total Messages/Day: 200M DAU * 40 msg/user = 8B messages&lt;/li&gt;
&lt;li&gt;Offline Message Ratio: 15%&lt;/li&gt;
&lt;li&gt;Daily Storage: 8B * 15% * 0.5KB = 0.6TB / Day&lt;/li&gt;
&lt;li&gt;Monthly Storage: 0.6TB * 30 = 18TB&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Technology Choice:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Hot Data (Recent)&lt;/strong&gt;: &lt;strong&gt;NoSQL (Cassandra / DynamoDB / HBase)&lt;/strong&gt;. Optimized for write-heavy workloads and time-series data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cold Data (History)&lt;/strong&gt;: Object Storage (S3) or Compressed Archives.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Offline Buffer&lt;/strong&gt;: Redis Streams or temporary KV storage to hold messages until the user comes online.&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>How to Design a POI (Point of Interest) Service</title>
        <link>https://sususu5.github.io/p/how-to-design-a-poi-point-of-interest-service/</link>
        <pubDate>Sat, 07 Jun 2025 11:43:38 +1000</pubDate>
        
        <guid>https://sususu5.github.io/p/how-to-design-a-poi-point-of-interest-service/</guid>
        <description>&lt;h1 id=&#34;how-to-design-a-poi-point-of-interest-service&#34;&gt;How to design a POI (Point of Interest) Service
&lt;/h1&gt;&lt;h2 id=&#34;functional-requirements&#34;&gt;Functional Requirements
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;Searching nearby locations&lt;/li&gt;
&lt;li&gt;Viewing details&lt;/li&gt;
&lt;li&gt;Owners can edit / delete&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;non-functional-requirements&#34;&gt;Non-functional Requirements
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;Latency: 1s&lt;/li&gt;
&lt;li&gt;Freshness: 1 day -&amp;gt; 1 hour&lt;/li&gt;
&lt;li&gt;Scalability&lt;/li&gt;
&lt;li&gt;Fault Tolerance&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;assumptions&#34;&gt;Assumptions
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;1B users -&amp;gt; 50% -&amp;gt; 500M DAU&lt;/li&gt;
&lt;li&gt;200M businesses&lt;/li&gt;
&lt;li&gt;Storage: NoSQL, SQL, in memory&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;database-selection&#34;&gt;Database Selection
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;In-memory: using multiple machines to shard the data, and the latency requirement is 1s, it&amp;rsquo;s expensive to use this&lt;/li&gt;
&lt;li&gt;SQL: more expensive&lt;/li&gt;
&lt;li&gt;NoSQL: cheaper, fast querying, self-sharding and optimization, suitable&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;how-to-scale-searching&#34;&gt;How to scale searching
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Master-Slave Model, separate reading and writing&lt;/li&gt;
&lt;li&gt;This system is suitable for using this model because the need for reading is much more than writing&lt;/li&gt;
&lt;li&gt;Not using whole table scaning here, given longtitude and latitude&lt;/li&gt;
&lt;li&gt;Composite Index can not speed up the searching here because it follows the &lt;strong&gt;Leftmost Prefix Rule&lt;/strong&gt;, we can only search the range of the latitude once the longitude is matched, so it equals whole table scaning&lt;/li&gt;
&lt;li&gt;Two other effective searching ways will be introduced in the following content&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;geohash&#34;&gt;GeoHash
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;GeoHash is a spatial hashing algorithm that continuously divides latitude and longitude ranges in half (using a Z-order curve), converting a (latitude, longitude) pair into a Base32 string that represents the geographic grid cell in which the location falls&lt;/li&gt;
&lt;li&gt;It is a value stored in the database&lt;/li&gt;
&lt;li&gt;The longer the string, the higher the precision of the geographical location&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;quad-tree&#34;&gt;Quad Tree
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;A Quad Tree is a tree data structure that recursively partitions a two-dimensional space into four quadrants (subregions). Each node in the tree has up to four children representing the top-left, top-right, bottom-left, and bottom-right regions&lt;/li&gt;
&lt;li&gt;It is a memory structure&lt;/li&gt;
&lt;li&gt;Quad Tree can be used as indicies to speed up the searching (k values), assume each node contains about 100 businesses&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Can it fits into memory?
(business_id, latitude, longitute, short_brief) &amp;lt; 50B
(200M / 100) * (100 * 50B) = 10GB, which can be stored in memory, meets the latency requirement in 1 second&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;How to build this tree?
Need a starting point, stored on disk as a seed value&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;How to update this tree?
Live update / eventually consistecy&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Adding a cache to store the latitude, longitude, and their corresponding business list, extra cost.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Construct the Quad Tree according to the deployment (west, middle, east USA), extra cost, availability&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://sususu5.github.io/images/sys_POI.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;System Architecture&#34;
	
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>How to Design a TikTok</title>
        <link>https://sususu5.github.io/p/how-to-design-a-tiktok/</link>
        <pubDate>Tue, 03 Jun 2025 16:18:56 +1000</pubDate>
        
        <guid>https://sususu5.github.io/p/how-to-design-a-tiktok/</guid>
        <description>&lt;h1 id=&#34;how-to-design-a-tiktok&#34;&gt;How to design a TikTok
&lt;/h1&gt;&lt;h2 id=&#34;functional-requirements&#34;&gt;Functional Requirements
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Video Upload&lt;/strong&gt;: Allow users to upload videos to the platform.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Video Playback&lt;/strong&gt;: Enable users to watch videos.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;By developing these, we can construct a minimum viable product.&lt;/p&gt;
&lt;h2 id=&#34;non-functional-requirements&#34;&gt;Non-functional Requirements
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: Support a massive user base with high concurrency.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Availability&lt;/strong&gt;: Ensure the system remains operational even during partial failures.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Low Latency&lt;/strong&gt;: Minimize delays in video loading and playback.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fault Tolerance&lt;/strong&gt;: Handle hardware/network failures gracefully without data loss.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;assumptions&#34;&gt;Assumptions
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;User Base&lt;/strong&gt;: 1 billion daily active users (DAU).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Usage Patterns&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;Each user watches 100 videos per day.&lt;/li&gt;
&lt;li&gt;Each user uploads 1 video per day.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Video Size&lt;/strong&gt;: Average video size is 10MB.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;database-selection-sql-vs-nosql&#34;&gt;Database Selection: SQL vs NoSQL
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;SQL Databases&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Pros&lt;/em&gt;: Strong consistency, relational data support.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Cons&lt;/em&gt;: Challenges with sharding and hotspot management.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NoSQL Databases&lt;/strong&gt;:
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Pros&lt;/em&gt;: Cost-effective, horizontally scalable.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Cons&lt;/em&gt;: Limited transactional support.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;video-storage-strategy&#34;&gt;Video Storage Strategy
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Blob Storage (Binary Large Object)&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Optimized for unstructured data (videos, images, audio).&lt;/li&gt;
&lt;li&gt;Ideal for storing and retrieving large volumes of small files efficiently.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;how-to-upload-a-video&#34;&gt;How to upload a video
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;Since we don&amp;rsquo;t know what users are uploading, exposing the storage to the interface directly is unsafe.&lt;/li&gt;
&lt;li&gt;A better option is allocating a temporary space to store the original videos uploaded by users.&lt;/li&gt;
&lt;li&gt;When uploading, a video can be cut into small pieces to support &lt;strong&gt;breakpoint resume upload&lt;/strong&gt; when a break happens, and also parallel uploading, which means multiple segments can be uploaded simultaneously. (for a mobile app, the network environment is not stable)&lt;/li&gt;
&lt;li&gt;Once all segments are uploaded, we can use a message queue and a worker pool to merge all the segments and do a file integrity verification. After that, this video should be encoded into different formats because videos of different qualities should be played according to devices and network.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;how-to-watch-a-video&#34;&gt;How to watch a video
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;To avoid hotspots caused by frequent access to popular videos, we can deploy a CDN near user locations to offload traffic from the blob storage.&lt;/li&gt;
&lt;li&gt;Although storing videos in a CDN speeds up delivery and reduces latency, it also comes with high costs. So, we should make sure only the most popular videos are cached there.&lt;/li&gt;
&lt;li&gt;By introducing an extractor service, it can regularly find popular videos from the blob storage and send them to the CDN, those outdated videos are replaced.&lt;/li&gt;
&lt;li&gt;We can also introduce a streaming protocol like the HTTP Live Streaming from Apple to realize &amp;ldquo;stream-as-you-go&amp;rdquo; manner, which improves user experience.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;show-off&#34;&gt;Show off
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;We can introduce a recommendation system to recommend videos to users rather than pushing original feed accoring to the time.&lt;/li&gt;
&lt;li&gt;We can introduce a Two-Tower Model to embed user and video features into separate vectors. When a client requests videos, the system can recommend those that match their interests based on vector similarity.&lt;/li&gt;
&lt;li&gt;The pro is the vieo watching time of clients can be extended, the con is the extra cost of hiring a team to construct and deploy the model.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://sususu5.github.io/images/sys_tiktok.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;System Architecture&#34;
	
	
&gt;&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
